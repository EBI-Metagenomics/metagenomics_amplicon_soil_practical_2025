[
  {
    "objectID": "sessions/qc.html",
    "href": "sessions/qc.html",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nCautionA note about copying and pasting in the VMs\n\n\n\nYou will not be able to use Ctrl+C for copying and Ctrl+P for pasting in the VM terminals. Instead, we recommend using right click and selecting copy/paste.\n\n\n\n\n\n\n\n\nNoteCheck data is loaded correctly\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data/. Check if it is there. If not, go to the Downloading data section\n\n\n\n\n\n\n\n\nCautionDownloading data\n\n\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data. If not, you should download and decompress the data as follows:\nmkdir -p /home/training/course_dir/data_dir\ncd /home/training/course_dir/data_dir\n\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz\n\ntar -xzvf qc_data.tar.gz\n# you should now have the quality subdirectory in your /home/training/QC_session. All the data you will need to run the practicals will be in this subdirectory\n\n#you can now remove the qc_data.tar.gz\nrm qc_data.tar.gz\n\n\n\n\n\n\n\n\n\nNoteKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/course_dir/work_dir/Day_2/qc_results\nYou should now have in your /home/training/course_dir directory, the following subdirectories (data_dir/qc_data/ and work_dir/Day_2/qc_results/). Make sure both are there before moving onto the next steps. You might run into permission issues later if you have not created them properly.\n\n\nFor this tutorial, you’ll need to move into the working directory (/home/training/course_dir/data_dir/qc_data) and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/data_dir/qc_data\nexport DATADIR=/home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/work_dir/Day_2/qc_results\nexport RESDIR=/home/training/course_dir/work_dir/Day_2/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host”\n\n\n\n\n\n\nImportant\n\n\n\nAgain, to avoid permission issues, it’s very important that the directories $DATADIR and $RESDIR variables are exported correctly before running the container. You can check this by running:\necho $DATADIR\necho $RESDIR\nThese commands should print out the paths for those variables. If it’s not printing anything, go back to the last instruction before proceeding.\n\n\nNow start the Docker container:\ndocker run --rm -it --user 1001 -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/metagenomics-qc-practical",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#prerequisites",
    "href": "sessions/qc.html#prerequisites",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nCautionA note about copying and pasting in the VMs\n\n\n\nYou will not be able to use Ctrl+C for copying and Ctrl+P for pasting in the VM terminals. Instead, we recommend using right click and selecting copy/paste.\n\n\n\n\n\n\n\n\nNoteCheck data is loaded correctly\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data/. Check if it is there. If not, go to the Downloading data section\n\n\n\n\n\n\n\n\nCautionDownloading data\n\n\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data. If not, you should download and decompress the data as follows:\nmkdir -p /home/training/course_dir/data_dir\ncd /home/training/course_dir/data_dir\n\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz\n\ntar -xzvf qc_data.tar.gz\n# you should now have the quality subdirectory in your /home/training/QC_session. All the data you will need to run the practicals will be in this subdirectory\n\n#you can now remove the qc_data.tar.gz\nrm qc_data.tar.gz\n\n\n\n\n\n\n\n\n\nNoteKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/course_dir/work_dir/Day_2/qc_results\nYou should now have in your /home/training/course_dir directory, the following subdirectories (data_dir/qc_data/ and work_dir/Day_2/qc_results/). Make sure both are there before moving onto the next steps. You might run into permission issues later if you have not created them properly.\n\n\nFor this tutorial, you’ll need to move into the working directory (/home/training/course_dir/data_dir/qc_data) and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/data_dir/qc_data\nexport DATADIR=/home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/work_dir/Day_2/qc_results\nexport RESDIR=/home/training/course_dir/work_dir/Day_2/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host”\n\n\n\n\n\n\nImportant\n\n\n\nAgain, to avoid permission issues, it’s very important that the directories $DATADIR and $RESDIR variables are exported correctly before running the container. You can check this by running:\necho $DATADIR\necho $RESDIR\nThese commands should print out the paths for those variables. If it’s not printing anything, go back to the last instruction before proceeding.\n\n\nNow start the Docker container:\ndocker run --rm -it --user 1001 -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/metagenomics-qc-practical",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "href": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Quality control and filtering of the raw sequencing read files",
    "text": "Quality control and filtering of the raw sequencing read files\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\nIn the following exercises, you’ll learn how to check the quality of short read sequences, identify adaptor sequences, remove adapters and low-quality sequences, and construct a reference database for host decontamination.\n\n\n\n\n\n\n\n\nNoteHere you should see the contents of the working directory.\n\n\n\nThese are the files we’ll use for the practical:\nls /opt/data\n\n\n\nQuality assessment with FastQC and multiqc\nWe will start by using a tool called FastQC, which will generate a report describing multiple quality measures for the given reads.\n\n\n\n\n\n\nStepGenerate a directory of the FastQC results\n\n\n\nmkdir -p /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_1_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_2_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\nThe -o option used with FastQC sends the output files to the given path.\n\n\n\n\n\n\nStepNow on your computer, select the folder icon.\n\n\n\nNavigate to Files → Home → course_dir → work_dir → Day_2 → qc_results → fastqc_results → oral in your VM\nRight-click on file oral_human_example_1_splitaa_fastqc.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of FastQC\n\n\nSpend some time looking at the ‘Per base sequence quality.’\n\n\nFor each position, a BoxWhisker-type plot is drawn:\n\nThe central red line is the median value.\nThe yellow box represents the inter-quartile range (25-75%).\nThe upper and lower whiskers represent the 10% and 90% points.\nThe blue line represents the mean quality.\n\nThe y-axis on the graph shows the quality scores. The higher the score, the better the base call. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). The quality of calls on most platforms will degrade as the run progresses, so it’s common to see base calls falling into the orange area towards the end of a read.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this tell you about your sequence data? When do the errors start?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nFrom the distribution of quality scores, we can tell that the sequencing for these reads is a standard good quality run, with the errors mainly starting at the 3’ end of the reads being a common feature.\n\n\n\nIn the pre-processed files, we see two warnings, as shown on the left side of the report. Navigate to the “Per bases sequence content.”\n\n\n\nScreenshot of FastQC\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAt around 15-19 nucleotides, the DNA composition becomes very even; however, at the 5’ end of the sequence, there are distinct differences. Why do you think that is?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThis bias at the beginning is a feature of the library preparation method used. In this case, a transposase-based fragmentation was used to break up the DNA for sequencing, which has biased towards certain bases.\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nOpen up the FastQC report corresponding to the reversed reads.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any significant differences between the forward and reverse files?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThere are no noticeable differences between the two directions, which is a good sign about the quality of the sequencing.\n\n\n\nFor more information on the FastQC report, please consult the ‘Documentation’ available from this site: FastQC Documentation\nWe are currently only looking at two files, but often we want to look at many files. The tool multiqc aggregates the FastQC results across many samples and creates a single report for easy comparison. Here we will demonstrate the use of this tool.\n\n\n\n\n\n\nStepRun\n\n\n\nmkdir -p /opt/results/multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/multiqc_results/oral\nchown 1001 /opt/results/multiqc_results/oral/*.html\n\n\nIn this case, we provide the folder containing the FastQC results to multiqc, and similar to FastQC, the -o argument allows us to set the output directory for this summarized report.\n\n\n\n\n\n\nStepNow on your computer, select the folder icon.\n\n\n\nNavigate to Home → course_dir → work_dir → Day_2 → qc_results → oral in your VM\nRight-click on file multiqc_report.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as two separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat fraction of reads are duplicates?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nFor both directions, the fraction of duplicate reads is 0.9%.\n\n\n\n\n\nQuality filtering with fastp\nSo far we have looked at the raw files and assessed their content, but we have not done anything about removing duplicates, sequences with low quality scores, or removal of the adaptors. So, let’s start this process.\nOur first step will be to perform quality filtering of the reads using a tool called fastp, which is versatile, easy to use, and fast.\n\n\n\n\n\n\nStepCreate directories that will store output files from the cleaning process\n\n\n\nmkdir -p /opt/results/cleaned/oral\n\n\nThe fastp command you will run contains multiple parameters, so let’s slowly deconstruct it:\n\n\n\n\n\n\nStepRun\n\n\n\nfastp --in1 /opt/data/oral_human_example_1_splitaa.fastq.gz \\\n      --in2 /opt/data/oral_human_example_2_splitaa.fastq.gz \\\n      --out1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/oral/oral.fastp.json --html /opt/results/cleaned/oral/oral.fastp.html\n\n\n\n--in1/--in2 — The two input paired-end read files\n--out1/--out2 — The two output files after filtering\n-l — Minimum read length required, reads below 50 in this case are discarded\n--cut_right/--cut_right_window_size/--cut_right_mean_quality — These three options all work together. --cut_right creates a window, of size specified by --cut_right_window_size, which will slide from the front to the tail of the reads, calculating the mean quality score in the window at every step. If at any point, the mean quality score value is lower than the one specified by --cut_right_mean_quality, then the bases of that window and everyting to its right are immediately discarded for that read.\n-t 1 — Will trim the tail of its final base, as it’s a lot lower quality than other positions. This is a setting you should set very purposefully and for good reason, like we’re doing here.\n--detect_adapter_for_pe — One of the very useful features of fastp is that it can detect adapters automatically and remove them, which this parameter activates.\n--json/--html — Outputs a summary report similar to FastQC, in both .json and .html formats.\n\n\n\n\n\n\n\nQuestion\n\n\n\nFind and open the .html report. How many reads were removed? How has the average quality of the reads changed? \n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nComparing the total reads metric in the before and after sections of the report, we can see that about 3.9 million reads were removed completely. The average quality of the reads has already increased, which can be seen from comparing the Q20/Q30 bases before and after filtering.\n\n\n\n\n\nDecontamination with bowtie2\nNext, we want to remove any potential contamination in our reads, which we’ll do using a tool called bowtie2. It is always good to routinely screen for human DNA (which may come from the host and/or staff performing the experiment). However, if the sample is from a mouse, you would want to download the mouse genome. The first step in the decontamination process is therefore to make a database that our reads will be searched against for sources of contamination.\nIn the following exercise, we are going to use two “genomes” already downloaded for you in the decontamination folder. To make this tutorial quicker and smaller in terms of file sizes, we are going to use PhiX (a common spike-in) and just chromosome 10 from human.\n\n\n\n\n\n\nNoteThe reference sequences files we’ll be using\n\n\n\nls /opt/data/decontamination\n\n# Output: GRCh38_chr10.fasta  phix.fasta\n\n\nFor the next step, we need one file, so we want to merge the two different fasta files. This is simply done using the command-line tool cat.\n\n\n\n\n\n\nStepRun\n\n\n\ncat /opt/data/decontamination/GRCh38_chr10.fasta /opt/data/decontamination/phix.fasta &gt; /opt/data/decontamination/chr10_phix.fasta\n\n\nYou will often need to build indices for large sequence files - including sequencing files and reference files - to speed up computation. To build a bowtie index for our new concatenated PhiX-chr10 file, run the following script. NOTE. The indexing step can take a while to run (~ 2 -3 minutes for the example used in this practical)\n\n\n\n\n\n\nStepRun\n\n\n\nbowtie2-build /opt/data/decontamination/chr10_phix.fasta /opt/data/decontamination/chr10_phix.index\n\n#check output - indexed files end with *bt2\nls /opt/data/decontamination/chr10_phix.index*bt2\n\n# opt/data/decontamination/chr10_phix.index.1.bt2\n# opt/data/decontamination/chr10_phix.index.2.bt2\n# opt/data/decontamination/chr10_phix.index.3.bt2\n# opt/data/decontamination/chr10_phix.index.4.bt2\n# opt/data/decontamination/chr10_phix.index.rev.1.bt2\n# opt/data/decontamination/chr10_phix.index.rev.2.bt2\n\n\n\n\n\n\n\n\nNoteTip\n\n\n\nIt is possible to automatically download a pre-indexed human genome in bowtie2. Commonly-used bowtie2 indices can be downloaded from https://bowtie-bio.sourceforge.net/bowtie2/index.shtml.\n\n\nNow we are going to use our new indexed chr10_phix reference and decontaminate our already quality-filtered reads from fastp. Run bowtie2 as below. NOTE. This alignment step can take a few minutes to run.\n\n\n\n\n\n\nStepRun\n\n\n\nbowtie2 -1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/oral/oral_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n-1 - input read 1\n-2 - input read 2\n-x - reference genome index filename prefix (minus trailing .X.bt2)\n--un-con-gz - write pairs that didn’t align concordantly to assigned filepath (this will be your cleaned reads)\n--very-sensitive - set stringent parameters to call reads as a mapped read (Same as -D 20 -R 3 -N 0 -L 20 -i S,1,0.50)\n--dovetail - concordant when mates extend past each other (ie. the paired alignments overlaps one another )\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the bowtie2 output on the terminal, what fraction of reads have been deemed to be contaminating?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nAs the bowtie2 output says, 3.15% reads have aligned to the reference, and are therefore contaminating.\n\n\n\nbowtie2 changes the naming scheme of the output files, so we rename them to be consistent:\n\n\n\n\n\n\nStepRun\n\n\n\nmv /opt/results/cleaned/oral/oral_human_example.fastq.1.gz /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/oral/oral_human_example.fastq.2.gz /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz\n\n\n\n\nPost-QC assessment with FastQC and multiqc\n\n\n\n\n\n\nStepRun FastQC\n\n\n\nUsing what you have learned previously, generate a FastQC report for each of the *trimmed_decontam.fastq.gz files. Output the new fastqc report files in the same /opt/results/fastqc_results/oral directory as last time.\n\n\n\n\n\n\n\n\nCautionRun FastQC (code)\n\n\n\n\n\nfastqc /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\n\n\n\n\n\n\n\nStepRun multiQC\n\n\n\nAlso generate a multiQC report, with /opt/results/fastqc_results/oral as input. The reason we generated the new FastQC reports in the same directory is so that you can compare how the reads have changed after the quality filtering and decontamination steps in the same final multiqc report.\nmkdir -p /opt/results/final_multiqc_results/oral\n&lt;you construct the command&gt;\n\n\n\n\n\n\n\n\nCautionRun multiQC (code)\n\n\n\n\n\nmkdir -p /opt/results/final_multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/final_multiqc_results/oral\n\n\n\n\n\n\n\n\n\nStepCheck report\n\n\n\nView the MultiQC report as before using your browser.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as four separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think of the change in sequence quality histograms? Have they improved?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe sequence quality histograms show clearly that the cleaned and decontaminated reads have improved quality throughout the reads now.\n\n\n\nThe reads have now been decontaminated and can be uploaded to ENA, one of the INSDC members. It is beyond the scope of this course to include a tutorial on how to submit to ENA, but there is additional information available on how to do this in this Online Training guide provided by EMBL-EBI",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#assembly-phix-decontamination",
    "href": "sessions/qc.html#assembly-phix-decontamination",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Assembly PhiX decontamination",
    "text": "Assembly PhiX decontamination\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\nIn the following exercises, you will generate a PhiX BLAST database and run a BLAST search with a subset of assembled freshwater sediment metagenomic reads to identify contamination.\n\n\nPhiX, used in the previous section of this practical, is a small bacteriophage genome typically used as a calibration control in sequencing runs. Most library preparations will use PhiX at low concentrations; however, it can still appear in the sequencing run. If not filtered out, PhiX can form small spurious contigs that could be incorrectly classified as diversity.\n\n\n\n\n\n\nStepGenerate the PhiX reference BLAST database:\n\n\n\nmakeblastdb -in /opt/data/decontamination/phix.fasta -input_type fasta -dbtype nucl -parse_seqids -out /opt/data/decontamination/phix_blastDB\n\n\nPrepare the freshwater sediment example assembly file and search against the new BLAST database. This assembly file contains only a subset of the contigs for the purpose of this practical.\n\n\n\n\n\n\nStepRun\n\n\n\nmkdir -p /opt/results/blast_results\ngunzip /opt/data/freshwater_sediment_contigs.fa.gz\nblastn -query /opt/data/freshwater_sediment_contigs.fa -db /opt/data/decontamination/phix_blastDB -task megablast -word_size 28 -best_hit_overhang 0.1 -best_hit_score_edge 0.1 -dust yes -evalue 0.0001 -min_raw_gapped_score 100 -penalty -5 -soft_masking true -window_size 100 -outfmt 6 -out /opt/results/blast_results/freshwater_blast_out.txt\n\n\nThe BLAST options are:\n\n-query — Input assembly fasta file.\n-out — Output file\n-db — Path to BLAST database.\n-task — Search type -“megablast”, for very similar sequences (e.g, sequencing errors)\n-word_size — Length of initial exact match\n\n\n\n\n\n\n\nStepAdd headers to the blast output and look at the contents of the final output file:\n\n\n\ncat /opt/data/blast_outfmt6.txt /opt/results/blast_results/freshwater_blast_out.txt &gt; /opt/results/blast_results/freshwater_blast_out_headers.txt\ncat /opt/results/blast_results/freshwater_blast_out_headers.txt\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre the hits significant?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe hits are very significant - they all have e-value hits of 0, which is the lowest it can go, and therefore the most confident the hit can be.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the lengths of the matching contigs? We would typically filter metagenomic contigs at a length of 500bp. Would any PhiX contamination remain after this filter?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe lengths of the matching contigs, in order, are: 596, 426, 389, and 385. You’d therefore still have the longest contig still present using this filter, as it’s 596 bases long.\n\n\n\nNow that PhiX contamination was identified, it is important to remove these contigs from the assembly file before further analysis or upload to public archives. You can either remove these matching contigs directly, or use a tool like bowtie2 to achieve this like you learned in the last section.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#using-controls",
    "href": "sessions/qc.html#using-controls",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Using Controls",
    "text": "Using Controls\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\nThis exercise will use 16S amplicon sequencing runs to look at the analysis of sequencing controls. Given two mystery sequencing runs, you will figure out which of the two is a control by comparing their microbial diversity, and also ascertain which kind of control it is.\n\n\n\nThe mystery sequencing runs\nGoing to /opt/data/mystery_reads, you will find two mystery FASTQ files. You have this information about them:\n\nThey are both amplicon sequencing runs.\nOne of the runs is a control\nOne of the runs is a mouse gut metagenome sequencing run of the 16S rRNA subunit.\n\nImagine you’re the one who sequenced these runs, and you want to analyse them, however you’ve forgotten which is the control. In this part of the practical, you will figure out which run was meant to be the control, and also the type of control it is, by using command-line tools to compare their microbial diversity.\nIn particular, you will use a tool called MAPseq and the 16S rRNA reference database SILVA to perform taxonomic classification of the sequencing reads. You will then be able to compare the assignments of the two sequencing runs to rediscover the purpose of each run.\n\n\n\n\n\n\nStepDownload the mystery sequencing runs into a new folder\n\n\n\nChange into the directory containing the mystery reads, and run ls -lh to see some information about the files.\ncd /opt/data/mystery_reads\nls -lh\n# total 7.7M\n# -rw-r--r-- 1 root root  26K Sep 11 12:24 mystery_reads_A.fastq.gz\n# -rw-r--r-- 1 root root 7.6M Sep 11 12:24 mystery_reads_B.fastq.gz\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCould you make an initial guess about which run is the control from this output?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe fifth column shows the file size, and it’s immediately noticeable how much smaller mystery_reads_A is compared to mystery_reads_B. This could indicate a lack of reads, which is either on purpose in the form of a control, or indicative of potential issues in sequencing.\n\n\n\n\n\nConverting FASTQ to FASTA with seqkit\nMAPseq requires the input reads to be in FASTA format rather than FASTQ. One very versatile and useful bioinformatics tool is SeqKit, which is a suite of commands for sequencing file manipulation.\n\n\n\n\n\n\nStepConvert FASTQ to FASTA with SeqKit\n\n\n\nThe SeqKit utility you want to use for the FASTQ-FASTA converstion is fq2fa. Go ahead and convert your FASTQ files like so\nseqkit fq2fa mystery_reads_A.fastq.gz -o mystery_reads_A.fasta.gz\nseqkit fq2fa mystery_reads_B.fastq.gz -o mystery_reads_B.fasta.gz\nYou will also need to uncompress the FASTA files before using MAPseq:\ngunzip mystery_reads_A.fasta.gz\ngunzip mystery_reads_B.fasta.gz\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThere’s another SeqKit utility that can generate summary metrics on the sequencing runs. Find it from the SeqKit documentation and use it on the mystery runs. What metric gives you another potential hint at the identity of the control?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe stats utility is very useful for getting a quick overview of sequence files:\nseqkit stats mystery_reads_*.fasta\n# file                      format  type  num_seqs     sum_len  min_len  avg_len  max_len\n# mystery_reads_A.fasta  FASTA   DNA        229      68,471      299      299      299\n# mystery_reads_B.fasta  FASTA   DNA     77,566  23,192,234      299      299      299\nThe num_seqs metric tells you how many sequences there are in each file. mystery_reads_A seems to have significantly less reads than mystery_reads_B, again hinting at it being a potential control, as you’d expect far more reads from an actual run.\n\n\n\n\n\nTaxonomic assignment of reads using MAPseq and SILVA\nNow that the reads are in the correct format, we can use MAPseq with the SILVA reference database to annotate the reads with taxonomic assignments.\n\n\n\n\n\n\nStepRun MAPseq on the reads\n\n\n\nThe SILVA reference database is already installed in the container at /SILVA-SSU/, so you can run MAPseq with the following commands:\nmapseq -seed 12 -tophits 80 -topotus 40 -outfmt simple mystery_reads_A.fasta /SILVA-SSU/SILVA-SSU.fasta /SILVA-SSU/SILVA-SSU-tax.txt &gt; mystery_reads_A.mseq\nmapseq -seed 12 -tophits 80 -topotus 40 -outfmt simple mystery_reads_B.fasta /SILVA-SSU/SILVA-SSU.fasta /SILVA-SSU/SILVA-SSU-tax.txt &gt; mystery_reads_B.mseq\nThis will generate two files of the .mseq format, which contain a taxonomy assignment for every read that had a match to the SILVA database. There is a lot of information about every matching read, such as matching coordinates, confidence, etc., but for our exercise we want to focus on the final column, which contains the actual assignment. Running these commands will aggregate that final column by computing a count of how many times each taxonomic assignment appears for both runs:\ntail -n +2 mystery_reads_A.mseq | cut -f14 | sort | uniq -c | awk '{print $1 \"\\t\" $2}' &gt; mystery_reads_A_counts.tsv\ntail -n +2 mystery_reads_B.mseq | cut -f14 | sort | uniq -c | awk '{print $1 \"\\t\" $2}' &gt; mystery_reads_B_counts.tsv\nFor example:\n23  sk__Bacteria;k__;p__Bacillota;c__Bacilli;o__Lactobacillales;f__Lactobacillaceae;g__Lactobacillus\nThis line means that this particular taxon was assigned to 23 different reads by MAPseq.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOn your virtual machine, open the two counts files you’ve just generated side-by-side, and compare them. What do you notice about the amount of unique assignments in both files? How about the counts? What is your final conclusion about which run is the control, and the kind of control it is?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nIt is clear that mystery_reads_A has significantly less microbial diversity than mystery_reads_B. In terms of richness i.e. how many unique taxa there are, mystery_reads_A only has 29, while mystery_reads_B has 123. In terms of abundance i.e. the total counts, only 229 reads were given a taxonomic assignment in mystery_reads_A, while mystery_reads_B has a total read count of 77,565.\nFrom these observations, it becomes clear that mystery_reads_A is the control, and that it is specifically a negative control. Indeed, mystery_reads_A has the accession SRR17185965, and is a water negative control sample, while mystery_reads_B has the accession SRR17185970, and is a mouse gut metagenome sample.\nNow that you know it’s a negative control, do you identify any taxa the samples have in common? Can you make any assertions about what this means?",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#additional-exercise",
    "href": "sessions/qc.html#additional-exercise",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Additional Exercise",
    "text": "Additional Exercise\nIf you have finished the practical you can try this step for more practice assessing and trimming datasets, there is another set of raw reads called “skin_example_aa” from the skin metagenome available. These will require a FastQC or multiqc report, followed by quality filtering and mapping to the reference database with fastp and bowtie2. Using what you have learned previously, construct the relevant commands. Remember to check the quality before and after trimming.\n\n\n\n\n\n\nStepNavigate to skin folder and run quality control.\n\n\n\nls /opt/data/skin\n# Output: skin_example_aa_1.fastq.gz  skin_example_aa_2.fastq.gz  skin_neg_control.fastq.gz\n\n\nRemember you will need to run the following command to view any html files in the VM browsers:\nchown 1001 foldername/*.html\n\n\n\n\n\n\nCautionAdditional Excercise (code)\n\n\n\n\n\n#generate fastqc of raw reads of skin samples and negative control\nmkdir /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_example_aa_1.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_example_aa_2.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_neg_control.fastq.gz -o /opt/results/fastqc_results/skin\n\n#do quality filtering using fastp \nmkdir /opt/results/cleaned/skin\nfastp --in1 /opt/data/skin/skin_example_aa_1.fastq.gz \\\n      --in2 /opt/data/skin/skin_example_aa_2.fastq.gz \\\n      --out1 /opt/results/cleaned/skin/skin_example_aa_1.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/skin/skin_example_aa_2.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/skin/oral.fastp.json --html /opt/results/cleaned/skin/skin.fastp.html\n\n#do host decontamination with bowtie2\nbowtie2 -1 /opt/results/cleaned/skin/skin_example_aa_1.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/skin/skin_example_aa_2.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/skin/skin_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n##decontaminated reads will be output as skin_human_example.fastq.1.gz and skin_human_example.fastq.2.gz in the /opt/results/cleaned/skin/ folder\n\n#rename decontaminated reads to be consistent\nmv /opt/results/cleaned/skin/skin_human_example.fastq.1.gz /opt/results/cleaned/skin/skin_human_example_1_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/skin/skin_human_example.fastq.2.gz /opt/results/cleaned/skin/skin_human_example_2_trimmed_decontam.fastq.gz\n\n#post-qc assessment with Fastqc and MultiQC\nfastqc /opt/results/cleaned/skin/skin_human_example_1_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/results/cleaned/skin/skin_human_example_2_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/skin\nchown 1001 /opt/results/fastqc_results/skin/*.html\n\n#generate MultiQC report of pre- and post-QC fastq files\nmultiqc /opt/results/fastqc_results/skin -o /opt/results/multiqc_results/skin\nchown 1001 /opt/results/fastqc_results/skin/*.html\n\n#visualise multiQC reports in web browser.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#running-the-practical-externally",
    "href": "sessions/qc.html#running-the-practical-externally",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Running the practical externally",
    "text": "Running the practical externally\nWe need to first fetch the practical datasets.\nmkdir QC_session\ncd QC_session\n\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz\n# or\nrsync -av --partial --progress rsync://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz .\nOnce downloaded, extract the files from the tarball:\ntar -xzvf qc_data.tar.gz\n\nrm qc_data.tar.gz\n\nmkdir qc_results\n\ncd qc_data\nNow pull the docker container and export the above directories.\ndocker pull quay.io/microbiome-informatics/metagenomics-qc-practical\nexport DATADIR={path to quality directory}\nexport RESDIR={path to qc_results directory}\nYou will see the message “access control disabled, clients can connect from any host”\ndocker run --rm -it  -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/metagenomics-qc-practical\nThe container has the following tools installed:\n\nfastqc\nmultiqc\nfastp\nbowtie2\nblast\nseqkit\nmapseq\n\nYou can now continue this practical from the section “Quality control and filtering of the raw sequence files”",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html",
    "href": "sessions/genome_uploader.html",
    "title": "The genome_uploader: tutorial",
    "section": "",
    "text": "This session will simulate genomes registration and submission with the genome_uploader, a tool developed within MGnify to facilitate the upload of bins and MAGs to the ENA (European Nucleotide Archive). If you are interested, you can find the genome_uploader code on github, on pypi and bioconda.\nWhen you submit genomes to the ENA, you need to register a sample for every genome containing all the relevant metadata describing the genome and the sample of origin. The genome_uploader acts as the main linker to preserve sample metadata as much as possible. For every genome to register, you need an INSDC run or assembly accession associated to the genome in order for the script to inherit its relevant metadata. On top of those metadata, the script adds metadata specified by the user that are specific to the genome, like taxonomy, statistics, or the tools used to generate it. The metadata that ENA requires are descibed in the checklist for MAGs and for bins, respectively.\nFirst, let’s access the exercise folder. Open a terminal and type:\nYou will find a starting dataset of two genomes. Together with them, input_example.tsv is a table containing metadata about those genomes. It will like similar to this:\nWith columns indicating:\nAccording to ENA checklist’s guidelines, broad_environment describes the broad ecological context of a sample - desert, taiga, coral reef, … local_environment is more local - lake, harbour, cliff, … environmental_medium is either the material displaced by the sample, or the one in which the sample was embedded prior to the sampling event - air, soil, water, … For host-associated metagenomic samples, the three variables can be defined similarly to the following example for the chicken gut metagenome: “chicken digestive system”, “digestive tube”, “caecum”. More information can be found at ERC000050 for bins and ERC000047 for MAGs under field names “broad-scale environmental context”, “local environmental context”, “environmental medium”\nIf your genome was generated from raw reads available on the INSDC (including ENA and GenBank), the genome_uploader will automatically inherit relevant metadata for that sample to make. For example, if you are submitting a MAG generated from read SRR11910206, some of the sample metadata will be inherited for the genome sample registration (e.g. collection_date, isolation_source).\nTake a look at the GCS MIMAG checklist as a reference. You will notice that bins and MAGs checklists are very similar, as mandatory fields are the same. You can compare it with the bins checklist yourself.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-and-submitting-genomes",
    "href": "sessions/genome_uploader.html#registering-and-submitting-genomes",
    "title": "The genome_uploader: tutorial",
    "section": "Registering and submitting genomes",
    "text": "Registering and submitting genomes\nAs explained before, you need to perform 4 steps to submit bins and MAGs:\n\nRegister a study\nRegister binned and/or MAG samples\nGenerate manifest files\nSubmit assemblies with Webin-CLI\n\nThe genome_uploader takes care of the last 3 steps. 2 and 3 are executed together, while the third one needs an extra command to submit previously generated files to ENA servers.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#accessing-yor-webin-profile",
    "href": "sessions/genome_uploader.html#accessing-yor-webin-profile",
    "title": "The genome_uploader: tutorial",
    "section": "Accessing yor Webin profile",
    "text": "Accessing yor Webin profile\nGo to this spreadsheet and reserve a Webin profile",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-a-study",
    "href": "sessions/genome_uploader.html#registering-a-study",
    "title": "The genome_uploader: tutorial",
    "section": "Registering a study",
    "text": "Registering a study",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-mag-samples-and-generating-manifest-files",
    "href": "sessions/genome_uploader.html#registering-mag-samples-and-generating-manifest-files",
    "title": "The genome_uploader: tutorial",
    "section": "Registering MAG samples and generating manifest files",
    "text": "Registering MAG samples and generating manifest files\nIt’s time to launch the genome_uploader.\n\n\n\n\n\n\nWarning\n\n\n\nReplace ENA_WEBIN and ENA_WEBIN_PASSWORD with your own credentials, and the METADATA_FILE and CENTRE_NAME parameters below with the right file ones!!\n\n\nexport ENA_WEBIN=\"Webin-XXX\"\nexport ENA_WEBIN_PASSWORD=\"Insert your password here\"\n\n# from the example_data folder:\n../genome_uploader/genomeuploader/genome_upload.py -u UPLOAD_STUDY --genome_info METADATA_FILE --mags --centre_name CENTRE_NAME\nWhere:\n\n-u UPLOAD_STUDY: study accession for genomes upload to ENA (in format ERPxxxxxx or PRJEBxxxxxx)\n---genome_info METADATA_FILE : genomes metadata file in tsv format\n--mags: replace --mags with --bins to upload bins: so EITHER OF THESE for either bin or MAG upload\n--centre_name CENTRE_NAME: name of the centre generating and uploading genomes\n\nThe logging output from this command will tell you which metadata objects have been accessed, and also where the output files have gone. This should be the example_data directory, but it will also have created a MAG_upload directory.\nBrowse the files here to see the manifests that have been created, which combine the ENA metadata and the provided metadata (e.g. binning software).\nYou can find the manifests in the MAG_upload/manifest_tests folder where you launched the script (should be in example_data).",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#submitting-your-genomes",
    "href": "sessions/genome_uploader.html#submitting-your-genomes",
    "title": "The genome_uploader: tutorial",
    "section": "Submitting your genomes",
    "text": "Submitting your genomes\nAfter checking that all needed manifests exist, it is necessary to use ENA’s webin-cli resource to upload genomes.\nFirst, download the resource in your folder.\ndownload_webin_cli -v 8.2.0\nThen, time to submit!\n\n\n\n\n\n\nWarning\n\n\n\nReplace MANIFEST_FILE below with the path to one of the manifest files that you just generated.\n\n\njava -jar webin-cli.jar -context=genome -manifest=MANIFEST_FILE -userName=${ENA_WEBIN} -password=${ENA_WEBIN_PASSWORD} -test -submit",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#try-it-yourself",
    "href": "sessions/genome_uploader.html#try-it-yourself",
    "title": "The genome_uploader: tutorial",
    "section": "Try it yourself",
    "text": "Try it yourself\n\n\n\n\n\n\nStep\n\n\n\nYour task is to generate another tsv table with sample metadata describing the genomes you want to register and submit.\n\n\nYou will be free to insert values as you wish, as long as regular expressions and mandatory fields described in the checklist are respected. You will need to select whether you are uploading bins or MAGs, and select your checklist accordingly.\n\n\n\n\n\n\nNote\n\n\n\nTo register a sample, a relative set of metadata must be filled according to the selected checklist. Some of them are mandatory, while some others are only recommended. The genome_uploader will automatically pick the right checklist depending on the input flag:\n\nGSC MIMAG for MAG samples (-mags)\nENA binned metagenome for binned samples (-bins)\n\n\n\nThe main difference between bins and MAGs lies in the uniqueness and the quality of your data. Within an ENA study, there should only be one MAG per species, which should be the highest quality representative genome per predicted species.\nHere we suggest a hypothetical scenario you might want to follow to make the metadata search more interesting\nSuppose your original dataset was small, very small, and it only generated three bins. Two of these bins represent the same species, but their statistics are extremely different. One assembled quite well, while the other one was highly contaminated. One would be considered “medium quality”, while the other “high quality”.\n\n\n\n\n\n\nTip\n\n\n\nThe INSDC defines a genome as high-quality when:\n\n\n\nAccording to what we previously mentioned, two of these bins could be categorised as MAGs, while the lower-quality bin would stay as a bin.\n\n\n\n\n\n\nTip\n\n\n\nTaxonomy lineages can be listed in either string (taxonomic names) or integer (taxids) format. An example of valid taxonomies you could use in this scenario could be:\n\nnames: d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Photobacterium;s__Photobacterium piscicola\nids: 1;131567;2759;33154;4751;451864;5204;452284;1538075;162474;742845;55193;76775",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#interested-in-retrieving-and-submitting-more-data-from-and-to-ena",
    "href": "sessions/genome_uploader.html#interested-in-retrieving-and-submitting-more-data-from-and-to-ena",
    "title": "The genome_uploader: tutorial",
    "section": "Interested in retrieving and submitting more data from and to ENA?",
    "text": "Interested in retrieving and submitting more data from and to ENA?\nHere is a link to a quick tour to use ENA for submission or retrieval of data.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/amplicon.html#download-an-amplicon-sample",
    "href": "sessions/amplicon.html#download-an-amplicon-sample",
    "title": "Amplicon stuff",
    "section": "Download an amplicon sample",
    "text": "Download an amplicon sample\nhttps://www.ebi.ac.uk/ena/browser/view/SRR5527446 –&gt; download selected files. We will focus on a sample from oilseed rape seeds.\nWhat is galaxy\nhttps://usegalaxy.org/\nCreate an account (give students 5 minutes)\nLog-in to your account.\nUpload the amplicon sample you downloaded.\n\n\n\nupload",
    "crumbs": [
      "Home",
      "Sessions",
      "Amplicon stuff"
    ]
  },
  {
    "objectID": "sessions/amplicon.html#fastqc",
    "href": "sessions/amplicon.html#fastqc",
    "title": "Amplicon stuff",
    "section": "FastQC",
    "text": "FastQC\nLook up FastQC in the list of tools\nFastq files contain the raw sequence returned by the sequencer, including quality values associated to every base pair.\nFastQC returns two types of outputs: a raw file and a pre-made html file containing multiple graphs with information about the sequence. Download that file, extract it, and open it in your browser of preference.\nPer base sequence quality is quite decent for all reads in the sample.\n\n\n\nfastqc_1\n\n\nYou can also look up the distribution of the average quality per read in the sample\n\n\n\nfastqc_2\n\n\nYou can observe how the distribution of amplicon bases is much more scattered compared to a metagenomic samples. This is usually because amplicon sequencing targets specific regions, leading to a higher chance to the same base position containing the same base.\n\n\n\nfastqc_3",
    "crumbs": [
      "Home",
      "Sessions",
      "Amplicon stuff"
    ]
  },
  {
    "objectID": "sessions/amplicon.html#convert-fastq-to-fasta",
    "href": "sessions/amplicon.html#convert-fastq-to-fasta",
    "title": "Amplicon stuff",
    "section": "Convert fastq to fasta",
    "text": "Convert fastq to fasta\nThis step is needed to proceed with downstream analyses. Fasta files are fastq files deprived of their quality values.\nLook up FASTQ to FASTA in the list of tools\nBeware of pointing to the correct fastq.gz file.",
    "crumbs": [
      "Home",
      "Sessions",
      "Amplicon stuff"
    ]
  },
  {
    "objectID": "sessions/amplicon.html#mapseq",
    "href": "sessions/amplicon.html#mapseq",
    "title": "Amplicon stuff",
    "section": "MAPseq",
    "text": "MAPseq\nLook up MAPseq in the list of tools\nLet’s select the SSU database, as this was the targeted subunit, and default parameters.\nSeparately we decided to run LSU and ITS databases, but the yield was very low, as expected.\nAmong the available options, there is the possibility of creating a table suitable for krona plot generation. This will be used later for a visual observation of your dataset.",
    "crumbs": [
      "Home",
      "Sessions",
      "Amplicon stuff"
    ]
  },
  {
    "objectID": "sessions/amplicon.html#kraken2",
    "href": "sessions/amplicon.html#kraken2",
    "title": "Amplicon stuff",
    "section": "Kraken2",
    "text": "Kraken2\nCHECK WHAT IS THE FASTEST, SILVA 138 16S or SILVA 2022\n25: 2022-02-02T162959Z_silva_kmer-len_35_minimizer-len_31_minimizer-spaces_6_load-factor_0.7 26: 16S_SILVA138_k2db2021-08\nLook up Kraken2 in the list of tools, let’s launch it with a default set of parameters, but only allowing for the best hit to be used. In a separate session you could change these parameters and aim for better accuracy, bearing in mind that execution time might increase.\n\nKraken-report + Krakentools\nA few more steps are required for kraken data to be converted to a format ingestable in a krona plot.\nFirst, select ‘Kraken-report’ from the list of tools, and give it the Kraken classification file as input. The tool will only run for a few seconds. Do the same with ’ Krakentools: Convert kraken report file to krona text file’ by giving it the file ‘Kraken-report’ just generated.\n\n\nKrona pie chart\nUse both tabular files that have been generated by MAPseq and Kraken2 as input files. This tool will generate a separate plot for each dataset.",
    "crumbs": [
      "Home",
      "Sessions",
      "Amplicon stuff"
    ]
  },
  {
    "objectID": "sessions/amplicon.html#kraken2-vs-mapseq-comparison",
    "href": "sessions/amplicon.html#kraken2-vs-mapseq-comparison",
    "title": "Amplicon stuff",
    "section": "Kraken2 vs MAPseq comparison",
    "text": "Kraken2 vs MAPseq comparison\nOpen the two krona plots you just generated. They should look like these two:\n\n\n\n\n\n\nMAPseq\n\n\n\n\n\n\n\nKraken2\n\n\n\n\n\nLook at the difference in annotation numbers.\nNotice the contamination on MAPseq’s output. Thinking about the parameters that we used, do you understand why this was picked up by MAPseq, but not by Kraken2?\n\n\nSee the answer\n\nKraken2 was only focused on a bacterial database, while we used a generic SSU database for MAPseq.\n\nLet’s focus on the bacterial annotations only.\n\n\n\nMAPseq_bac",
    "crumbs": [
      "Home",
      "Sessions",
      "Amplicon stuff"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metagenomics bioinformatics at MGnify (2025)",
    "section": "",
    "text": "This is the course material for the Metagenomics bioinformatics at MGnify course (2025).\n\nOverview\nGain knowledge of the tools, processes and analysis approaches used in the field of metagenomics.\nThis course will cover the metagenomics data analysis workflow from the point of newly generated sequence data. Participants will explore the use of publicly available resources and tools to manage, share, analyse and interpret metagenomics data. The content will include issues of data quality control and how to submit to public repositories. While sessions will detail marker-gene and whole-genome shotgun (WGS) approaches; the primary focus will be on assembly-based approaches. Discussions will also explore considerations when assembling genome data, the analysis that can be carried out by MGnify on such datasets, and what downstream analysis options and tools are available"
  },
  {
    "objectID": "sessions/assemblies.html",
    "href": "sessions/assemblies.html",
    "title": "Assembly and Co-assembly of Metagenomic Raw Reads",
    "section": "",
    "text": "Learning Objectives\nIn the following exercises, you will learn how to perform metagenomic assembly and co-assembly, and explore the output. We will visualize assembly graphs with Bandage, examine assembly statistics with assembly_stats, and align contigs against the BLAST database.\n\n\n\n\n\n\nNote\n\n\n\nMetagenomic assembly can take hours or even days to complete on real samples, often requiring days of CPU time and hundreds of gigabytes of memory. In this practical, we will work with small toy datasets to demonstrate the assembly process.\n\n\nOnce you have quality-filtered your sequencing reads, you may want to perform de novo assembly in addition to, or as an alternative to, read-based analyses. The first step is to assemble your sequences into contigs. There are many tools available for this purpose, such as metaSPAdes, IDBA-UD, metaMDBG, and MEGAHIT.\nWe generally use metaSPAdes, as it typically yields the best contig size statistics (i.e., more contiguous assembly) and has been shown to capture high degrees of community diversity (Vollmers, et al. PLOS One 2017). However, you should consider the pros and cons of different assemblers based on your specific needs. Key factors include not only assembly accuracy but also computational overhead, which varies significantly between tools.\nFor example, metaSPAdes requires substantial memory, especially for very diverse samples with large amounts of sequence data (e.g., soil samples). In such cases, more memory-efficient alternatives like MEGAHIT may be preferable.\nIn the following practicals, we will demonstrate the use of metaSPAdes on a small short-read sample, Flye on a long-read sample, and MEGAHIT for co-assembly.\n\n\nBefore we start…\nFirst, let’s navigate to the root working directory where we will run all analyses:\ncd /home/training/Assembly/\nThe raw reads for assembly generation can be found in the data_dir/short_reads, data_dir/long_reads and data_dir/co_assembly_short_reads folders.\n\n\nShort-read assemblies: metaSPAdes\nFor short reads, we will use SPAdes - St. Petersburg genome Assembler, a suite of assembly tools designed for different types of sequencing data. For metagenomic data, we will use the metagenomic-specific mode of SPAdes called metaSPAdes.\nmetaSPAdes offers many options to fit different requirements, which depend primarily on the properties of data you want to assemble and desired result. Keep in mind that options differ between tools (e.g., spades.py vs metaspades.py).\n\n\n\n\n\n\nStep\n\n\n\nTo explore available options of metaSPAdes, type:\nmetaspades.py -h\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat metaSPAdes command should be used for the case described below?\nYou need to assemble paired-end reads from the short_reads folder and save the output to a folder named assembly_spades. Limit resource usage to 5 GB of memory and 4 threads. By default, metaSPAdes performs an error correction on the input reads before assembling. Since provided reads have already been polished, you can run the assembler without the error correction step.\nReview the help output from metaspades.py -h and select the appropriate options to build your first assembly command.\n\n\n\n\nClick to see solution\n\nThe correct metaSPAdes parameters are:\n\n-1 short_reads/input_1.fastq: path to forward reads\n-2 short_reads/input_2.fastq: path to reverse reads\n-o assembly_spades: output folder name\n-t 4: use 4 threads\n-m 5: limit memory usage to 5 Gb\n--only-assembler: skips the error correction step\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease do not execute the assembly command on your machine. Execution time would overall cover half of the session (metaSPAdes alone would take ~15 minutes). Moreover, the VMs have limited resources that barely meet metaSPAdes’ requirements, and running multiple assemblies simultaneously could overload the system.\n\n\nAll output files (including intermediate ones) can be found in the assembly_spades folder.\ncontigs.fasta and scaffolds.fasta are typically used for downstream analyses (e.g., binning and MAG generation). We will focus on contigs.fasta for this session, which is the same file you will use in the upcoming practicals.\nWithout going all the way to MAG generation, you can sometimes identify strong taxonomic signals at the assembly stage using a quick blastn alignment.\n\n\n\n\n\n\nStep\n\n\n\nOpen blast website and choose Nucleotide BLAST (blastn). Take the first 100 lines of the sequence by copying the output of the command below:\nhead -n 101 assembly_spades/contigs.fasta\nLeave all other options as default on the search page and run BLAST search. The resulting output will have the following format (but will look a bit better than this): \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBased on the top BLAST hits, what species does this contig appear to originate from?\nThis read dataset was sequenced from a human oral sample. Is this organism commonly found in the human oral microbiome?\n\n\nAs mentioned in the theory session, you will need different statistics to evaluate the quality of your assembly.\nassembly_stats is a tool that produces two simple tables in JSON format with various measures, including N10 to N50, GC content, longest contig length, and more. The first section of the JSON output corresponds to scaffolds, while the second corresponds to contigs.\nN50 is a metric used to describe the quality of assembled genomes that are fragmented in contigs of different length. It represents the sequence length of the shortest contig at 50% of the total assembly length (when contigs are sorted from longest to shortest).\nA (hopefully) clarifying picture to understand N50, where N50==60: \nEssentially, a higher N50 value indicates a better assembly, as it means that longer contigs cover half of the final assembly, making it less fragmented. However, keep in mind that using N50 to compare the quality of two metagenomes only makes sense if the metagenomes are similar.\nNote that other metrics follow the same principle: for example, N90 represents the shortest contig length needed to cover 90% of the metagenome.\n\n\n\n\n\n\nStep\n\n\n\nYou can run assembly_stats with the following command:\nassembly_stats assembly_spades/scaffolds.fasta\n\n\nYou will see a summary output with statistics for your assembly. In lines with the format N50 = YYY, n = Z, the value n represents the number of sequences needed to cover 50% of the total assembly length. A “gap” is any consecutive run of Ns (undetermined nucleotide bases) of any length. N_count is the total number of Ns across the entire assembly.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the lengths of the longest and shortest contigs?\nWhat is the N50 of the assembly? Considering that the input consisted of ~150 bp paired-end reads, what do these statistics tell you about the quality of the assembly?\n\n\nAnother useful tool for assessing metagenomic assemblies is QUAST, which provides deeper insights into assembly statistics — such as indel frequency and misassembly rate — in just a short amount of time.\n\n\nLong-read assemblies: Flye\nFor long reads, we will use Flye, an assembler designed for single-molecule sequencing reads such as those produced by PacBio and Oxford Nanopore Technologies (ONT). Like SPAdes, Flye performs both reads error correction and assembly. It compensates for the high base-calling error rate in long reads by comparing multiple reads that cover the same genomic region.\n\n\n\n\n\n\nStep\n\n\n\nYou can view all Flye parameters using the help command:\nflye -h\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat Flye command should be used for the case described below?\nYou need to assemble ONT reads from the long_reads folder using 4 threads and save the output to a folder named assembly_flye_meta. The provided ONT reads are raw and require error correction. Also, keep in mind that by default, Flye performs an isolate assembly, but in this case, we want to perform a metagenomic assembly.\n\n\n\n\nClick to see solution\n\nThe correct Flye parameters are:\n\n--nano-raw long_reads/ONT_input.fastq: assemble input reads with error correction. Depending on data quality, other presets can be used. For example, if reads have been pre-polished and adapters removed, use --nano-corr. The --nano-hq option should be reserved for high-quality data. The same logic applies to the pacbio options.\n--threads 4: number of threads\n--out-dir assembly_flye_meta: output folder\n--meta: enable metagenome mode\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs before, we recommend to NOT launch this command during this session. Each run takes around 5 minutes and would fully occupy all available CPU cores.\n\n\nThe output files generated by Flye can be found in the assembly_flye_meta folder.\n\n\n\n\n\n\nNote\n\n\n\nTo further investigate the effect of the --meta flag, we also generated another assembly without it. The output of that run can be found in the assembly_flye folder.\n\n\n\n\nDiving Into Assembly Graphs\nLet’s take a first look at what assembly graphs actually look like. Bandage (short for Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a program that creates interactive visualisations of assembly graphs. It can be used to explore different parts of the graph — for example, rRNA genes, SNPs, or inspect specific genomic regions. With it, you can zoom and pan around the graph, search for sequences, and perform many other operations.\nIn this exercise, we will use Bandage to compare the assembly graphs we generated with Flye and metaSPAdes. We will start with metaSPAdes.\nWhen working with metaSPAdes output, it is usually recommended to open the file assembly_graph.fastg in Bandage. However, our assembly is quite fragmented, so in this case we will load assembly_graph_after_simplification.gfa instead.\n\n\n\n\n\n\nStep\n\n\n\nExecute the following command to lauch the Bandage application:\nBandage &\nIn the Bandage graphical interface, perform the following steps:\n\nGo to File → Load graph\nNavigate to /home/training/Assembly/assembly_spades\nSelect and open the file assembly_graph_after_simplification.gfa\n\nOnce the file is loaded, you need to draw the graph. To do so, in the Graph drawing panel on the left side perform the following:\n\nSet Scope to Entire graph\nClick on Draw graph\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you identify any large or complex regions within the metaSPAdes graph? If so, what do they look like?\n\n\nNow, open another instance of Bandage as you did before, and load assembly_flye/assembly_graph.gfa.\n\n\n\n\n\n\nQuestion\n\n\n\nHow does the Flye assembly differ from the one generated with metaSPAdes?\n\n\nAs mentioned earlier, we ran Flye both with and without the --meta flag on long_reads/ONT_input.fastq. You can now repeat the same visualization procedure for the other Flye graph and compare the results.\nNotice how the metagenomic assembly mode affects the graph structure compared to the default isolate assembly mode.\n\n\n\n\n\n\nQuestion\n\n\n\nThis ONT dataset originates from run ERR3775163, which can be explored on the NCBI browser. Take a look at its metadata.\nCan you figure out why the assembly graph from the run without --meta looks better than the one with --meta?\n\n\n\n\n\n\n\n\nNoteExtra\n\n\n\nTry running a BLAST search for the first 100 lines of the first contig of the long-read assembly. (You won’t be able to BLAST the entire contigs as they are too long.) Do the results match the metadata you found on ENA?\n\n\n\n\nCo-assemblies: MEGAHIT\nIn the next part of this exercise, we will explore co-assembly of multiple datasets. Remember that co-assembly produces meaningful results only when applied to similar samples. This is true for the raw reads we co-assembled here — they all originate from a single sample that was split for this exercise.\n\n\n\n\n\n\nStep\n\n\n\nExplore MEGAHIT parameters using the help command:\nmegahit -h\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs before, we recommend not to launch MEGAHIT command this time.\n\n\nYou will find MEGAHIT output files in the co_assembly_short_reads folder.\nThe following parameters were used to generate the co-assemblies:\n\n-1 [forward files comma-separated]\n-2 [reverse files comma-separated]\n-o co_assembly_megahit output folder\n-t 4 number of threads\n--k-list 23,51,77 list of k-mer lengths\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompare the co-assembly contigs file with the single-assembly one using the assembly_stats tool.\nHow does this assembly differ from the one generated earlier with metaSPAdes? Which one do you think is better, and why?\n\n\n\n\n\n\n\n\nNoteExtra\n\n\n\nYou might notice that MEGAHIT does not generate assembly graphs by default. To create one, run the following command:\nmegahit_toolkit contig2fastg 77 co_assembly_megahit/final.contigs.fa &gt; co_assembly_megahit/final.contigs.fastg\nExplore the graph in Bandage.\nThe datasets used for the metaSPAdes assembly and the MEGAHIT co-assembly originate from the same source. Do you observe any relevant differences between the two assembly graphs?\n\n\n\n\nThe End\nFantastic work — you’ve reached the end of the practical! 🎉\nYou’ve explored different assemblers, compared their outputs, and learned how to visualise and interpret assembly graphs — great progress toward mastering metagenomics.\nTake a moment to reflect on what you’ve learned today.\nIf anything remains unclear, don’t hesitate to ask questions — there are no silly questions!\nIf you still have some spare time, take a look at the sections labelled “Extra.” They contain optional exercises for curious students who want to explore further.\n\n\n\n\nReuseApache 2.0",
    "crumbs": [
      "Home",
      "Sessions",
      "Assembly and Co-assembly of Metagenomic Raw Reads"
    ]
  },
  {
    "objectID": "sessions/metagenomics.html",
    "href": "sessions/metagenomics.html",
    "title": "Metagenomics stuff",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nReuseApache 2.0",
    "crumbs": [
      "Home",
      "Sessions",
      "Metagenomics stuff"
    ]
  }
]